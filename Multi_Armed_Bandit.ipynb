{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc6a23-3eed-4499-90cd-53e0505a80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Lab 6: Multi-Armed Bandit'''\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings in output\n",
    "\n",
    "# Import standard random number generator and NumPy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seed values for reproducibility of results\n",
    "random.seed(1693)       # Fix Python's random module seed\n",
    "np.random.seed(1693)    # Fix NumPy's random number generator seed\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Q6-0: Initialize problem environment\n",
    "# -----------------------------------------------\n",
    "\n",
    "num_arms = 5  # Specify the number of arms (actions) available to the agent\n",
    "probabilities = list(np.random.rand(num_arms))  # Generate random float probabilities [0,1) for each arm\n",
    "\n",
    "print(probabilities)  # Q6-0: Print list of generated arm probabilities\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Define reward function (environment model)\n",
    "# -----------------------------------------------\n",
    "\n",
    "def reward(prob):               # Define reward generator function based on arm probability\n",
    "    n_iterations = 10           # Number of times to simulate the outcome (like pulling same arm 10 times)\n",
    "    reward = 0                  # Initialize total reward to 0\n",
    "    for i in range(n_iterations):         # Repeat for n_iterations\n",
    "        if random.random() > prob:        # Simulate stochastic reward: reward = 1 if random > probability\n",
    "            reward += 1                   # Add 1 to reward if condition met\n",
    "    return reward              # Return the total reward for the arm\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Q6-1: Evaluate initial reward for each arm\n",
    "# -----------------------------------------------\n",
    "\n",
    "for i in range(num_arms):                             # Iterate through each arm index\n",
    "    r = reward(probabilities[i])                      # Call reward function using arm's probability\n",
    "    print(f'The total reward for arm {i} is {r}')     # Q6-1: Print reward for each arm\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Q6-2: Initialize memory array (action-value)\n",
    "# -----------------------------------------------\n",
    "\n",
    "starting_arm = 3                                      # Choose initial action index (arm 3)\n",
    "av = np.array([starting_arm, 0]).reshape(1,2)         # Initialize memory array: action=3, reward=0, shape=(1,2)\n",
    "\n",
    "print(av)  # Q6-2: Print initialized memory array\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Q6-3: Define function to choose best arm\n",
    "# -----------------------------------------------\n",
    "\n",
    "def bestArm(memory):                  # Define function to select best arm based on memory array\n",
    "    arm_rewards = {}                 # Dictionary to track cumulative reward per arm\n",
    "    arm_counts = {}                 # Dictionary to track number of times each arm is selected\n",
    "\n",
    "    for record in memory:           # Iterate through each record in memory array\n",
    "        arm = record[0]             # Extract arm index\n",
    "        rew = record[1]             # Extract reward received\n",
    "\n",
    "        if arm not in arm_rewards:  # Initialize tracking if arm is new\n",
    "            arm_rewards[arm] = rew\n",
    "            arm_counts[arm] = 1\n",
    "        else:                       # Accumulate reward and count if arm already tracked\n",
    "            arm_rewards[arm] += rew\n",
    "            arm_counts[arm] += 1\n",
    "\n",
    "    best_arm = 0                    # Default to arm 0 as best\n",
    "    best_avg = -1                   # Start with lowest possible average\n",
    "\n",
    "    for arm in arm_rewards:         # Loop through each tracked arm\n",
    "        avg = arm_rewards[arm] / arm_counts[arm]  # Compute mean reward for this arm\n",
    "        if avg > best_avg:                      # If better than current best, update\n",
    "            best_avg = avg\n",
    "            best_arm = arm\n",
    "\n",
    "    return best_arm  # Return the index of the best-performing arm so far\n",
    "\n",
    "print(f'The best arm is #{bestArm(av)}')  # Q6-3: Print best arm based on initial memory\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Q6-4: Run epsilon-greedy simulation\n",
    "# -----------------------------------------------\n",
    "\n",
    "n_trials = 10       # Set number of simulations (trials)\n",
    "epsilon = 0.25      # Exploration rate: 25% explore, 75% exploit\n",
    "total_reward = 0    # Track total reward across trials\n",
    "\n",
    "for i in range(n_trials):               # Loop over each trial\n",
    "    if random.random() > epsilon:      # With 75% probability (1 - epsilon), exploit\n",
    "        arm = bestArm(av)              # Select best arm based on memory\n",
    "    else:                              # Otherwise (25%), explore\n",
    "        arm = random.randint(0, num_arms - 1)  # Pick a random arm\n",
    "\n",
    "    r = reward(probabilities[arm])     # Get reward from chosen arm using reward function\n",
    "\n",
    "    new_record = np.array([arm, r]).reshape(1, 2)  # Format new action-reward record as 1x2 array\n",
    "    av = np.vstack((av, new_record))              # Append new record to memory array\n",
    "\n",
    "    total_reward += r                 # Update cumulative reward\n",
    "    cumulative_mean = total_reward / (i + 1)  # Compute average reward so far\n",
    "\n",
    "    print(cumulative_mean)  # Q6-4: Print cumulative mean reward for this trial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612987b-cfd6-44b6-a8ac-cf7c79c214fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Assignment 6: Multi-Armed Bandit'''\n",
    "# Suppress TensorFlow logs and warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # \"Set environment variable to suppress TF logs\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set seeds for reproducibility (assignment uses fixed seeds)\n",
    "random.seed(1693)          # Ensures repeatable results from random module\n",
    "np.random.seed(1693)       # Ensures repeatable results from NumPy\n",
    "tf.random.set_seed(1693)   # Ensures repeatable results from TensorFlow\n",
    "\n",
    "# ----------------------------------------\n",
    "# Initialize 2-armed bandit environment\n",
    "# ----------------------------------------\n",
    "\n",
    "n = 2  # Number of arms (\"Specify number of actions/arms\")\n",
    "\n",
    "# Define arms as a 2D NumPy array where each row represents an arm\n",
    "# Each arm has a [mean, standard deviation] for its normal reward distribution\n",
    "arms = np.array([[3,1], [6,2]])  # Arm 0: mean=3, sd=1; Arm 1: mean=6, sd=2\n",
    "\n",
    "print(f'0: ', arms)  # Q6-0: Print the initialized arms with their parameters\n",
    "\n",
    "# ----------------------------------------\n",
    "# Reward function using normal distribution\n",
    "# ----------------------------------------\n",
    "\n",
    "def reward(dist):\n",
    "    mean, sd = dist                        # Unpack mean and standard deviation of selected arm\n",
    "    zscore = np.random.normal(0, 1, 1)     # Draw a random sample from standard normal (mean=0, sd=1)\n",
    "    score = mean + zscore * sd             # Convert z-score to a value from N(mean, sd)\n",
    "    return score                           # Return reward value\n",
    "\n",
    "print(f'1: ', reward([5,1]))  # Q6-1: Print a sample reward from an arm with mean=5, sd=1\n",
    "\n",
    "# ----------------------------------------\n",
    "# Initialize memory (action-value) array\n",
    "# ----------------------------------------\n",
    "\n",
    "starting_arm = 0            # Start with arm 0\n",
    "starting_reward = 0         # Initial reward is 0\n",
    "av = np.array([starting_arm, starting_reward]).reshape(1,2)  # Reshape to 1x2 array\n",
    "\n",
    "print(f'2: ', av)  # Q6-2: Print memory with the first action-reward pair\n",
    "\n",
    "# ----------------------------------------\n",
    "# Define function to return best arm so far\n",
    "# ----------------------------------------\n",
    "\n",
    "def bestArm(a):\n",
    "    bestArm = 0         # Initialize best arm as 0\n",
    "    bestMean = 0        # Initialize best mean reward as 0\n",
    "\n",
    "    for u in a: \n",
    "        this_action = a[np.where(a[:,0] == u[0])]  # Get all records for current arm u[0]\n",
    "        avg = np.mean(this_action[:, 1])           # Calculate average reward for this arm\n",
    "\n",
    "        if bestMean < avg:                         # If this arm's avg reward is better...\n",
    "            bestMean = avg                         # ...update bestMean\n",
    "            bestArm = u[0]                         # ...update bestArm index\n",
    "\n",
    "    return bestArm  # Return the arm with the highest observed average reward\n",
    "\n",
    "# ----------------------------------------\n",
    "# Run epsilon-greedy simulation\n",
    "# ----------------------------------------\n",
    "\n",
    "n_trials = 20   # Total number of learning iterations\n",
    "eps = 0.7       # Probability of exploration (i.e., choosing a random arm)\n",
    "\n",
    "for i in range(n_trials):\n",
    "    if random.random() > eps:  # 30% of the time (1 - epsilon), do exploitation\n",
    "        choice = bestArm(av)   # Choose best arm based on history\n",
    "    else:                      # 70% of the time, do exploration\n",
    "        choice = np.random.randint(0, n)  # Randomly select an arm\n",
    "\n",
    "    score = reward(arms[choice])              # Get reward using current arm's distribution\n",
    "    thisAV = np.array([choice, score]).reshape(1,2)  # Format new action-reward pair\n",
    "    av = np.concatenate((av, thisAV), axis=0)        # Append to memory array\n",
    "    runningMean = np.mean(av[:,1])                   # Calculate cumulative mean reward\n",
    "\n",
    "    print(f'3: ', runningMean)  # Q6-3: Print current mean reward after each trial\n",
    "\n",
    "print(av)  # Q6-4: Print the final memory array after all trials\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
